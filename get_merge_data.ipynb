{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Add path for python to look into for modules installed using pip\n",
    "\n",
    "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n",
    "\n",
    "import quandl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "quandl.ApiConfig.api_key = \"n2tNssPxEFC9-Ad79fo-\" # keep this private\n",
    "\n",
    "figsize = (15, 8)\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ticker/RIC codes from different stock exchanges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Codes(stock_exchange):\n",
    "    \n",
    "    data = pd.read_csv(str(stock_exchange)+\".csv\", encoding='unicode_escape')\n",
    "    company_name = []\n",
    "\n",
    "    for i in data[\"Symbol\"]:\n",
    "    \n",
    "        company_name.append(i)\n",
    "\n",
    "    codes = np.asarray(company_name)\n",
    "    \n",
    "    return codes\n",
    "\n",
    "\n",
    "Get_Codes(\"NYSE\")\n",
    "\n",
    "Get_Codes(\"NASDAQ\")\n",
    "\n",
    "Get_Codes(\"ASE\")\n",
    "\n",
    "# We must add the codes at the end to make it compatible with Refinitiv data\n",
    "\n",
    "def Add_Extension(codes, ext):\n",
    "    \n",
    "    newcodes = []\n",
    "    \n",
    "    for i in codes:\n",
    "        \n",
    "        newcodes.append(str(i) + str(ext))\n",
    "        \n",
    "    new = np.asarray(newcodes)\n",
    "    \n",
    "    return new   \n",
    "\n",
    "Add_Extension(Get_Codes(\"NASDAQ\"), ext=\".O\")\n",
    "\n",
    "# We can now create a list of extensions to call the above function for each stock exchange:\n",
    "\n",
    "# list can be found at: \n",
    "# http://training.thomsonreuters.com/portal/docs/pdf/raymondjames/Thomson_One_Exchange_List.pdf\n",
    "\n",
    "# For the time being we have the 3 largest American Stock Exchanges:\n",
    "\n",
    "# - NYSE: .N\n",
    "# - NASDAQ: .O\n",
    "# - ASE: .A\n",
    "\n",
    "# Get Quandl data\n",
    "\n",
    "NYSE_quandl = list(Get_Codes(\"NYSE\"))\n",
    "NASDAQ_quandl = list(Get_Codes(\"NASDAQ\"))\n",
    "ASE_quandl = list(Get_Codes(\"ASE\"))\n",
    "\n",
    "# Get Rifinitiv data\n",
    "\n",
    "NYSE_rifinitiv = list(Add_Extension(Get_Codes(\"NYSE\"), ext=\".N\"))\n",
    "NASDAQ_rifinitiv = list(Add_Extension(Get_Codes(\"NASDAQ\"), ext=\".O\"))\n",
    "ASE_rifinitiv = list(Add_Extension(Get_Codes(\"ASE\"), ext=\".A\"))\n",
    "\n",
    "print('NYSE_quandl:', len(NYSE_quandl))\n",
    "print('NASDAQ_quandl:', len(NASDAQ_quandl))\n",
    "print('ASE_quandl:', len(ASE_quandl))\n",
    "\n",
    "print('NYSE_rifinitiv:', len(NYSE_rifinitiv))\n",
    "print('NASDAQ_rifinitiv:', len(NASDAQ_rifinitiv))\n",
    "print('ASE_rifinitiv:', len(ASE_rifinitiv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start/end dates\n",
    "\n",
    "start = '2010-10-15'\n",
    "end = '2018-10-15'\n",
    "\n",
    "oos_start = end\n",
    "oos_end = '2019-10-15'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rifinitiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Rifinitiv #####\n",
    "\n",
    "# The following values are populated for you by Data Science Accelerator. \n",
    "# They represent your demo-level access to the data.\n",
    "# Please don't share this with anyone\n",
    "\n",
    "# RESOURCE_ENDPOINT = 'https://dsa-stg-edp-api.fr-nonprod.aws.thomsonreuters.com/data/environmental-social-governance/v1/views/scores-full'\n",
    "access_token = 'uGR7cxvvqJ4mgWwva5pPN184iGGigBhY8g4ThFu0' # personal key for Data Science Accelerator access to ESG\n",
    "\n",
    "def get_data_request(url, requestData):\n",
    "    '''HTTP GET request'''\n",
    "    dResp = requests.get(url, headers = {'X-api-key': access_token}, params = requestData);       \n",
    "\n",
    "    if dResp.status_code != 200:\n",
    "        raise ValueError(\"Unable to get data. Code %s, Message: %s\" % (dResp.status_code, dResp.text));\n",
    "    else:\n",
    "        print(\"Data access successful\")\n",
    "        jResp = json.loads(dResp.text);\n",
    "        return jResp\n",
    "\n",
    "def get_data(ric):\n",
    "    '''Gets ESG scores for a specific RIC (company) code'''\n",
    "    \n",
    "    requestData = {\n",
    "    \"universe\": ric\n",
    "    };\n",
    "\n",
    "    jResp = get_data_request(RESOURCE_ENDPOINT, requestData)\n",
    "\n",
    "    data = jResp[\"data\"]\n",
    "    headers = jResp[\"headers\"]    \n",
    "\n",
    "    names = [headers[x]['title'] for x in range(len(headers))]\n",
    "\n",
    "    df = pd.DataFrame(data, columns=names )\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Updating the API endpoint \n",
    "\n",
    "RESOURCE_ENDPOINT = 'https://dsa-stg-edp-api.fr-nonprod.aws.thomsonreuters.com/data/environmental-social-governance/v1/views/measures-full'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read available Quandl ticker symbols\n",
    "\n",
    "tickers = pd.read_csv('ticker_list.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define essential loop function\n",
    "\n",
    "def aggregate(asset1, asset2, start_date=start, end_date=end):\n",
    "    \n",
    "    # Quandl\n",
    "    \n",
    "    if 'EOD/'+asset1.replace('.', '_') in list(tickers['Quandl_Code']):\n",
    "        \n",
    "        try:\n",
    "            quandl_data = quandl.get('EOD/'+asset1.replace('.', '_'), start_date=start_date, end_date=end_date)\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "        # Get stocks with at least X Quandl entries\n",
    "        \n",
    "        if len(quandl_data) >= 500:\n",
    "            quandl_data['log_ret'] = np.log(quandl_data.Adj_Close) - np.log(quandl_data.Adj_Close.shift(1))\n",
    "            quandl_data['Date'] = pd.to_datetime(quandl_data.index, format='%Y-%m-%d')\n",
    "            quandl_data['Year'] = quandl_data['Date'].dt.year\n",
    "            quandl_yearly = quandl_data.groupby(['Year']).mean()\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    # Refinitiv\n",
    "    \n",
    "    try:\n",
    "        refinitiv_data = get_data(asset2)\n",
    "    except:\n",
    "        return 0\n",
    "        \n",
    "    if len(refinitiv_data) != 0:\n",
    "        refinitiv_data['Date'] = pd.to_datetime(refinitiv_data['Period End Date'], format='%Y-%m-%d')\n",
    "        refinitiv_data['Year'] = refinitiv_data['Date'].dt.year\n",
    "        refinitiv_data['Period End Date'] = pd.to_datetime(refinitiv_data['Period End Date'], format='%Y-%m-%d')\n",
    "        refinitiv_data = refinitiv_data[(refinitiv_data['Period End Date'] >= start) & \n",
    "                                        (refinitiv_data['Period End Date'] <= end)]\n",
    "        refinitiv_data.set_index('Year', inplace=True)\n",
    "        refinitiv_data.dropna(subset=['TRESG Combined Score'], inplace=True)\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "    # Join datasets (if both available)\n",
    "    \n",
    "    all_data = quandl_yearly.join(refinitiv_data, on='Year', how='left')\n",
    "    \n",
    "    return all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quandl-Rifinitive loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "\n",
    "index = random.sample(population=range(0, len(NASDAQ_quandl)), k=n)\n",
    "\n",
    "assets_quandl = [NASDAQ_quandl[i] for i in index]\n",
    "\n",
    "assets_ref = [NASDAQ_rifinitiv[i] for i in index]\n",
    "\n",
    "datasets = []\n",
    "\n",
    "i = 1\n",
    "\n",
    "for asset1, asset2 in zip(assets_quandl,assets_ref):\n",
    "    \n",
    "    print(i, '/', n, ':', asset1, ',', asset2)\n",
    "    \n",
    "    data_tmp = aggregate(asset1, asset2)\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    if isinstance(data_tmp, pd.DataFrame):\n",
    "        datasets.append(data_tmp)\n",
    "        print(data_tmp.shape, '\\n')\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(datasets) >= 1:\n",
    "    \n",
    "    # Get all column names\n",
    "\n",
    "    col_names = []\n",
    "\n",
    "    for d in datasets:\n",
    "        col_names.append(d.columns)\n",
    "    \n",
    "    # Intersection of list of lists\n",
    "\n",
    "    unique_colnames = list(set.intersection(*map(set, col_names)))\n",
    "    \n",
    "else:\n",
    "    print('No datasets found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset dataframes in datasets\n",
    "\n",
    "new_data = []\n",
    "\n",
    "for d in datasets:\n",
    "    df_tmp = d[np.intersect1d(d.columns, unique_colnames)]\n",
    "    df_tmp = df_tmp.loc[:, ~df_tmp.columns.duplicated()]\n",
    "    new_data.append(df_tmp)\n",
    "    \n",
    "# Merge dataframes in new_data list\n",
    "\n",
    "all_data = pd.concat(new_data, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data.to_csv('aggr_data.csv', sep = ',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
